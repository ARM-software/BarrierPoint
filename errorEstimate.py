#!/usr/bin/env python

# Copyright (c) 2020, Arm Limited and Contributors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Module to parse performance statistics collected with PAPI, validate the
# output files and estimate the average reconstruction error between the
# selected BPs and the whole application. It also generates error plots.

import logging
import os
import sys
import argparse
import csv
import json
from collections import defaultdict
from itertools import product
import matplotlib.pyplot as plt
import numpy as np

import matplotlib
matplotlib.use('Agg')


class errorEstimate(object):
    def __init__(self, fbaseline, gbarriers, bbpoints, prfcntrs_suf):

        self.logger = logging.getLogger('errorEstimate')
        # bperf check CSV file (generated by PAPI)
        self.fbaselineFN = fbaseline
        # bperf check BP CSV file (per BP) (generated by PAPI)
        self.gbarriersFN = gbarriers
        self.bbpointsP = bbpoints  # Barriers folder (generated by DR)
        self.prfcntrs_suf = prfcntrs_suf
        # Storing the barrier points
        self.BPs = defaultdict(list)
        self.BPs_mult = defaultdict(list)
        # Storing the stats
        self.STATSt = []
        self.STATSbp = []

        # Parse all the subdirectory output paths for all the barrier points repetitions
        # (files generated by DR, located in outputs/barriers)
        _subdirs = [x[0]
                    for x in os.walk(self.bbpointsP) if os.path.isdir(x[0])][1:]
        self.logger.debug("Found {} BP iterations".format(len(_subdirs)))

        _bpsFN = []
        for _sd in _subdirs:
            _bpsFN.append([y for y in [x[2] for x in os.walk(_sd)]
                           [0] if y.endswith('.barrierpoints')][0])

        for p, f in zip(_subdirs, _bpsFN):
            bbpointsFN = '{}/{}'.format(p, f)
            try:
                self.b = open(bbpointsFN, 'r')
                self.logger.debug(
                    'Parsing barrierpoints file: {}'.format(bbpointsFN))

                bps_id = int(p.split('/')[-1])

                for line in self.b:
                    _line = line.split(' ')
                    self.BPs[bps_id].append(int(_line[0]))
                    self.BPs_mult[bps_id].append(float(_line[1].strip()))

                self.logger.debug('Found {} barrier points: {}'.format(
                    len(self.BPs[bps_id]), ','.join(str(x) for x in self.BPs[bps_id])))
                self.logger.debug('Multipliers: {}'.format(
                    ','.join(str(x) for x in self.BPs_mult[bps_id])))

            except OSError as err:
                self.logger.error(
                    '{0}  bbpointsFN: {1}'.format(err, bbpointsFN))
                return

        # Parsing fbaseline (performance statistics of whole app)
        try:
            self.f = open(self.fbaselineFN, 'r')
            self.logger.debug(
                'Parsing baseline file: {}'.format(self.fbaselineFN))

            csvreader = csv.reader(self.f)
            # Read header line of CSV file
            if sys.version_info[0] < 3:  # Python cross-compatible
                stat = csvreader.next()[4:]
            else:
                stat = csvreader.__next__()[4:]

            for _line in csvreader:
                self.miniApp = _line[0]
                tid = int(_line[1])
                itr = int(_line[2])
                bid = int(_line[3])

                # Create a dict of stats per thread (taken from .csv)
                # Each statistic has a list of values correspoding to the iterations
                if len(self.STATSt) < (tid + 1):
                    self.STATSt.append(defaultdict(list))
                for i, value in enumerate(_line[4:]):  # Stats
                    if len(self.STATSt[tid][stat[i]]) < (itr + 1):
                        self.STATSt[tid][stat[i]].append([])
                    self.STATSt[tid][stat[i]][itr].append(float(value))
                    assert len(self.STATSt[tid][stat[i]][itr]) == (bid + 1)

            self.iterations = len(self.STATSt[tid]['Cycles'])
            self.nthreads = len(self.STATSt)

        except OSError as err:
            self.logger.error(
                '{}  fbaselineFN: {}'.format(err, self.fbaselineFN))
            return

        # Parsing gbarriers (performance statistics per BP)
        try:
            self.g = open(self.gbarriersFN, 'r')
            self.logger.debug(
                'Parsing baseline file: {}'.format(self.gbarriersFN))

            # Read header line of CSV file
            csvreader = csv.reader(self.g)
            if sys.version_info[0] < 3:  # Python cross-compatible
                stat = csvreader.next()[4:]
            else:  # Python3
                stat = csvreader.__next__()[4:]

            for _line in csvreader:
                tid = int(_line[1])
                itr = int(_line[2])
                bid = int(_line[3])

                # Create a dict of stats per thread (taken from .csv)
                # Each statistic has a list of values correspoding to the iterations
                if len(self.STATSbp) < (tid + 1):
                    self.STATSbp.append(defaultdict(list))
                for i, value in enumerate(_line[4:]):
                    if len(self.STATSbp[tid][stat[i]]) < (itr + 1):
                        self.STATSbp[tid][stat[i]].append([])
                    self.STATSbp[tid][stat[i]][itr].append(float(value))
                    assert len(self.STATSbp[tid][stat[i]][itr]) == (bid + 1)

            # Last line always contains the highest Barrier ID
            self.barriers = bid

        except OSError as err:
            self.logger.error(
                '{0}  gbarriersFN: {1}'.format(err, self.gbarriersFN))
            return

    def Metric(self, metric, tid, bid=0):
        # Returns the average across iterations and stdev for thread tID of a given metric
        values = list(map(lambda x: x[bid], self.STATSt[tid][metric]))
        return np.mean(values), np.std(values, ddof=1)

    def EstMetric(self, metric, tid, bps_id=0):
        # Returns the estimated average across iterations and stdev for thread tID of a given metric
        values = [0.0 for _ in range(self.iterations)]

        for i, bp in enumerate(self.BPs[bps_id]):
            if bp > len(self.STATSbp[tid][metric][0]):
                continue
            values = [x + y for x, y in zip(values, map(
                lambda x: self.BPs_mult[bps_id][i] * x[bp], self.STATSbp[tid][metric]))]

        return np.mean(values), np.std(values, ddof=1)


def plotError(plots_data, outpath, debug, suffix, plot_format):
    # Plots the reconstruction error estimations for different metrics
    # and different BP iterations. It generates an error graph for each
    # BP identification suffix (it compares all the performance statistic suffixes to it)
    # and for each BP repetition previously generated.
    # Each graph will contain an error bar (and std dev) for each metric and
    # for each suffix of the target application.

    logger = logging.getLogger('errorEstimate')
    logger.info("Plotting Error")

    # Check plot format (available formats: pdf, png)
    if plot_format != "pdf" and plot_format != "png":
        logger.warn(
            "Format {} not supported for the plot figures. \
Please choose between png or pdf. Using pdf format.".format(plot_format))
        plot_format = "pdf"

    # Number of plots in the figure (N rows, 2 columns for better visibility)
    num_sets = len(plots_data[0].BPs)
    n_x = np.ceil(num_sets / 2)
    n_y = 2

    # Metrics to plot
    metrics = ['Cycles', 'Instructions', 'L1D', 'L2D']
    metrics_labels = ['Cycles', 'Instrs', 'L1DMiss', 'L2DMiss']

    # Set plot figure size
    # How many bars per metric in the plots
    num_bars = len(plots_data)
    width_sep = 0.9
    # The more bars in the plot, the wider the figure (better readability)
    fig_width = 12. + 0.5 * num_bars
    fig = plt.figure(figsize=(fig_width, 12.))
    fig.subplots_adjust(hspace=0.5)
    ax = []
    rect = []
    hatches = [' ', '///', '---', '\\\\\\']
    index = 0  # Index for labeling the error bars with the error values

    # Get the number of repetions
    for i, bps in enumerate(sorted(plots_data[0].BPs.keys())):
        ax.append(fig.add_subplot(n_x, n_y, i + 1))
        logger.info("i: {}, bps: {}".format(i, bps))
        ind = np.arange(len(metrics))  # X positions of the plot
        width = width_sep / num_bars  # Width of the bars

        # Multiple bars (one for each perfcntrs suffix) for each of the four metrics
        for j, m in enumerate(metrics):
            logger.info("Metrics: {}".format(m))
            for k, suf in enumerate(plots_data):
                Yl, Yerrl = [], []
                # Calculate the average values of all threads for the metrics
                for tid in range(suf.nthreads):
                    if m == 'Cycles' or m == 'Instructions':
                        b, berr = suf.Metric(m, tid)
                        y, yerr = suf.EstMetric(m, tid, bps_id=bps)
                    else:
                        b, berr = suf.Metric(m + 'Misses', tid)
                        y, yerr = suf.EstMetric(m + 'Misses', tid, bps_id=bps)

                    logger.info("b={}, y={}".format(b, y))

                    Yl.append(abs(y / b - 1) * 100)
                    Yerrl.append(100 * yerr / b)

                rect.append(ax[i].bar(x=ind[j] + k * width, height=np.mean(Yl),
                                      width=width, edgecolor='black', hatch=hatches[k % 4], yerr=max(Yerrl)))

                # Error numbers at the top of the plot bars (Uncomment to include them)
                # height = rect[index][0].get_height()
                # ax[i].text(rect[index][0].get_x() + rect[index][0].get_width()/2.,
                # 1.05*height, '%.2f' % (np.mean(Yl)), ha='center', va='bottom')
                # index += 1

            # Resets the color cycle for each metric
            ax[i].set_prop_cycle(None)

        # Pretty axis
        ymin, ymax = ax[i].get_ylim()
        if ymax < 1.5:
            ax[i].set_ylim(0.0, 1.5)
        else:
            ax[i].set_ylim(0.0, ymax)
        plt.grid(True)

        # Center xtick for any number of bars
        ax[i].set_xticks(ind + (width_sep / 2) - (width / 2))
        ax[i].set_xticklabels(metrics_labels)
        ax[i].set_ylabel('Error [%]')
        axtitle = 'BarrierPoint Set {}'.format(i)
        ax[i].set_title(axtitle, fontsize=11)

    # Legend placement
    _legend = [plots_data[i].prfcntrs_suf for i, k in enumerate(plots_data)]
    plt.legend(_legend, title="Perfcntr runs", loc="center",
               bbox_to_anchor=(-0.13, -0.5), ncol=4)
    plt.suptitle('Estimation Errors\n(Barrierpoint identification: {}.{}t-{})'.
                 format(plots_data[0].miniApp, plots_data[0].nthreads, suffix), fontsize=18)

    # Set figure name and save it
    figname = '{}/{}.{}t.Error-{}.{}'.format(
        outpath, plots_data[0].miniApp, plots_data[0].nthreads, suffix, plot_format)

    logger.info("Figname: {}".format(figname))

    if not os.path.exists(os.path.dirname(figname)):
        try:
            os.makedirs(os.path.dirname(figname))
        except OSError:
            logger.error(
                "CSV files do not have a match rep number {} != {}".format(figname))
            sys.exit()

    fig.savefig(figname, format=plot_format, bbox_inches='tight')

    fig.clf()
    plt.close()


def find_csv_file(name, path):
    logger = logging.getLogger('errorEstimate')

    for root, dirs, files in os.walk(path):
        files = [f for f in files if not f[0] == '.']  # ignore hidden files
        # Look for the *.check.csv files
        check_csv = [s for s in files if name in s and "BP" not in s]
        if len(check_csv) == 0:
            logger.error("CSV file `xx.{}.csv` not found in {}".format(name, path))
            sys.exit()

        BP_check_csv = [s for s in files if name in s and "BP" in s]
        if len(BP_check_csv) == 0:
            logger.error("CSV file `xx.BP.repX.{}.csv` not found in {}".format(name, path))
            sys.exit()

        # Sanity check - repetition number in the filename has to match
        rep = [s for s in check_csv[0].split('.') if "rep" in s]
        rep_bp = [s for s in BP_check_csv[0].split('.') if "rep" in s]
        if int(rep[0][3:]) != int(rep_bp[0][3:]):
            logger.error("CSV files do not have a match rep number {} != {}".format(
                check_csv, BP_check_csv))
            sys.exit()
        return check_csv[0], BP_check_csv[0]


def main():

    parser = argparse.ArgumentParser()
    parser.add_argument('-c', '--config_file',
                        required=True, help='JSON config file')
    # parser.add_argument('-m', '--metrics', default=0, help='Plot Metrics per BP')

    args = parser.parse_args()

    # Decode JSON configuration file
    with open(args.config_file, 'r') as f:
        json_data = json.load(f)

    # rootdir = json_data["paths"]["rootdir"]
    outpath = json_data["paths"]["outpath"]
    plotpath = json_data["paths"]["plotpath"]
    apps = json_data["Application"]
    nthreads = json_data["threads"]
    plot_format = json_data["plot_format"]
    debug_mode = json_data["Debug"]

    # Define the logger (debug is written to an out file)
    formatter = logging.Formatter('[%(asctime)s] [%(levelname)s] - (%(name)s) - %(message)s')
    handler = logging.FileHandler("debug-errorEstimate.log", "w")
    handler.setFormatter(formatter)

    # Console handler (only INFO level)
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    ch.setFormatter(formatter)

    logger = logging.getLogger("errorEstimate")
    if debug_mode:
        logger.setLevel(logging.DEBUG)
    else:
        logger.setLevel(logging.INFO)

    # Add handlers to logger
    logger.addHandler(handler)
    logger.addHandler(ch)

    # Generate error plots for every combination of applications and threads
    for (ap, nt) in product(apps, nthreads):
        logger.info("== Plotting {}.{}t".format(ap, nt))

        # Detect errors in the configuration file
        if "Barriers_suffix" not in apps[ap]:
            logger.error(
                "Barriers_suffix parameter not found in the configuration file for {}".format(ap))
        elif "Perfcntrs_suffix" not in apps[ap]:
            logger.error(
                "Perfcntrs_suffix parameter not found in the configuration file for {}".format(ap))
        if not apps[ap]['Barriers_suffix']:
            logger.warn("Barriers_suffix list is empty for {}".format(ap))
        elif not apps[ap]['Perfcntrs_suffix']:
            logger.warn("Perfcntrs_suffix list is empty for {}".format(ap))

        # Generate plots for each identified BP set (marked by different suffixes)
        for bp in apps[ap]['Barriers_suffix']:
            logger.debug("= Barrier suffix: {}".format(bp))

            plots_data = []
            bp_filename = "{}-{}.{}t".format(ap, bp, nt)
            bbpoints = "{}/barriers/{}".format(outpath, bp_filename)

            # Plot the reconstruction error estimate (selected BPs vs. whole app)
            # of each perfcntr run against the set of identified BPs
            for prfcntrs in apps[ap]['Perfcntrs_suffix']:
                logger.debug("= Perfcntr suffix: {}".format(prfcntrs))
                perf_filename = "{}-{}.{}t".format(ap, prfcntrs, nt)
                perf_dir = "{}/bperf.{}/{}".format(outpath,
                                                   prfcntrs, perf_filename)
                # Check the output directory for the perfcntrs CSV files
                fbaseline, gbarriers = find_csv_file("check", perf_dir)
                fbaseline = "{}/{}".format(perf_dir, fbaseline)
                gbarriers = "{}/{}".format(perf_dir, gbarriers)
                logger.debug("\n> fbaseline: {}\n> gbarriers: {}\n> bbpoints: {}".format(
                    fbaseline, gbarriers, bbpoints))

                plots_data.append(errorEstimate(
                    fbaseline, gbarriers, bbpoints, prfcntrs))

            plotError(plots_data, plotpath, debug_mode, bp, plot_format)


if __name__ == '__main__':
    main()
